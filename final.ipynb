{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_13380\\229336683.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "#Library\n",
    "\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import make_scorer, precision_score\n",
    "\n",
    "from hazm import Normalizer, word_tokenize, stopwords_list\n",
    "import re\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import files\n",
    "train_df = pd.read_csv('data_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hazm dictionary has been used for pre-processeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = train_df['comment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove any special characters and symbols\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "comments = comments.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Tokenization and Normalization\n",
    "normalizer = Normalizer()\n",
    "comments = comments.apply(normalizer.normalize)\n",
    "comments = comments.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Stopword Removal\n",
    "stop_words = set(stopwords_list())\n",
    "comments = comments.apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['comment'] = comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the tokenized words into sentences, otherwise models can not work\n",
    "train_df['comment'] = train_df['comment'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As last column is Str, we need to ecnode it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns for each intent class and set initial value to 0\n",
    "new_df = pd.DataFrame(train_df)\n",
    "\n",
    "\n",
    "intent_classes = ['Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5']\n",
    "for class_label in intent_classes:\n",
    "    new_df[class_label] = 0\n",
    "\n",
    "# Update the columns based on the intent values\n",
    "for index, row in new_df.iterrows():\n",
    "    intents = row['intent'].split(',')\n",
    "    for intent in intents:\n",
    "        new_df.at[index, f'Class {intent}'] = 1\n",
    "\n",
    "# Drop the original intent column\n",
    "new_df.drop(columns=['intent'], inplace=True)\n",
    "\n",
    "train_df = new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.metrics import Precision\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df\n",
    "X = df['comment'].values\n",
    "y = df[['Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Tokenization\n",
    "max_words = 10000  # Adjust as needed\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_seq = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Padding\n",
    "maxlen = 100  # Adjust as needed\n",
    "X_pad = pad_sequences(X_seq, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define the CNN architecture\n",
    "embedding_dim = 100  # Adjust as needed\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(5, activation='sigmoid'))  # Sigmoid activation for multi-label classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[Precision()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1620/1620 [==============================] - 115s 71ms/step - loss: 0.1907 - precision_1: 0.9089 - val_loss: 0.2653 - val_precision_1: 0.8683\n",
      "Epoch 2/2\n",
      "1620/1620 [==============================] - 115s 71ms/step - loss: 0.1663 - precision_1: 0.9261 - val_loss: 0.2884 - val_precision_1: 0.8537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x137d86fdf90>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Train the model\n",
    "model.fit(X_train, y_train, epochs=2, batch_size=40, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 3s 5ms/step - loss: 0.2870 - precision_1: 0.8464\n",
      "Test Precision: 0.8464247584342957\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Evaluate the model\n",
    "loss, precision = model.evaluate(X_test, y_test)\n",
    "print(\"Test Precision:\", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15336956</td>\n",
       "      <td>خیلی خوبه عالیه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15336959</td>\n",
       "      <td>زیبا بود</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15336960</td>\n",
       "      <td>به علت شکیتگی مرجوع کردم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15336961</td>\n",
       "      <td>هم جعبه ماوس باز شده بود و هم ماوس شکسته بود</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15336964</td>\n",
       "      <td>چراغ قوه اش خوب بود</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>14426574</td>\n",
       "      <td>من سایز ۱۸ تا ۲۴ ماه سفارش دادم اما وقتی رسید ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>14426637</td>\n",
       "      <td>ولی این بار برای من اشتباه ارسال شده بود</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>14426701</td>\n",
       "      <td>متاسفاته من مهتابی سفارش داده بودم ولی برام آف...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>14426742</td>\n",
       "      <td>دقیقاهمین چیزی که داخل عکسه ر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>14426779</td>\n",
       "      <td>همین الان به دستم رسید . برخلاف کیف های دیگه ک...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                            comment\n",
       "0     15336956                                    خیلی خوبه عالیه\n",
       "1     15336959                                           زیبا بود\n",
       "2     15336960                           به علت شکیتگی مرجوع کردم\n",
       "3     15336961       هم جعبه ماوس باز شده بود و هم ماوس شکسته بود\n",
       "4     15336964                                چراغ قوه اش خوب بود\n",
       "...        ...                                                ...\n",
       "8995  14426574  من سایز ۱۸ تا ۲۴ ماه سفارش دادم اما وقتی رسید ...\n",
       "8996  14426637           ولی این بار برای من اشتباه ارسال شده بود\n",
       "8997  14426701  متاسفاته من مهتابی سفارش داده بودم ولی برام آف...\n",
       "8998  14426742                      دقیقاهمین چیزی که داخل عکسه ر\n",
       "8999  14426779  همین الان به دستم رسید . برخلاف کیف های دیگه ک...\n",
       "\n",
       "[9000 rows x 2 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('data_test_users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = test_df['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = test_comments.apply(clean_text)\n",
    "# Step 2: Tokenization and Normalization\n",
    "test_comments = test_comments.apply(normalizer.normalize)\n",
    "test_comments = test_comments.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Stopword Removal\n",
    "test_comments = test_comments.apply(lambda x: [word for word in x if word not in stop_words])\n",
    "test_df['comment'] = test_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the tokenized words into sentences, otherwise models can not work\n",
    "test_df['comment'] = test_df['comment'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Create columns for each intent class and set initial value to 0\n",
    "new_df_test = pd.DataFrame(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocess the test data\n",
    "test_data = new_df_test['comment'].values  # assuming df_test is your test dataframe\n",
    "test_data_seq = tokenizer.texts_to_sequences(test_data)\n",
    "test_data_pad = pad_sequences(test_data_seq, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 3s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Use the trained model to predict the labels for the test data\n",
    "predictions_test = model.predict(test_data_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert the predicted probabilities into binary classes based on a threshold\n",
    "threshold = 0.5\n",
    "binary_predictions_test = (predictions_test > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes for the test data:\n",
      "[[1 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " ...\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Display or further analyze the predicted classes\n",
    "print(\"Predicted classes for the test data:\")\n",
    "print(binary_predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to avoid mistake, we again make a data frame from test data.\n",
    "test_df_original = pd.read_csv('data_test_users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id                                            comment  Class 1  \\\n",
      "0     15336956                                    خیلی خوبه عالیه        1   \n",
      "1     15336959                                           زیبا بود        0   \n",
      "2     15336960                           به علت شکیتگی مرجوع کردم        0   \n",
      "3     15336961       هم جعبه ماوس باز شده بود و هم ماوس شکسته بود        1   \n",
      "4     15336964                                چراغ قوه اش خوب بود        0   \n",
      "...        ...                                                ...      ...   \n",
      "8995  14426574  من سایز ۱۸ تا ۲۴ ماه سفارش دادم اما وقتی رسید ...        0   \n",
      "8996  14426637           ولی این بار برای من اشتباه ارسال شده بود        0   \n",
      "8997  14426701  متاسفاته من مهتابی سفارش داده بودم ولی برام آف...        0   \n",
      "8998  14426742                      دقیقاهمین چیزی که داخل عکسه ر        0   \n",
      "8999  14426779  همین الان به دستم رسید . برخلاف کیف های دیگه ک...        1   \n",
      "\n",
      "      Class 2  Class 3  Class 4  Class 5  \n",
      "0           0        0        0        0  \n",
      "1           0        0        0        0  \n",
      "2           0        0        0        0  \n",
      "3           0        0        0        0  \n",
      "4           0        0        0        0  \n",
      "...       ...      ...      ...      ...  \n",
      "8995        0        1        0        0  \n",
      "8996        0        0        1        0  \n",
      "8997        0        0        1        0  \n",
      "8998        0        0        1        0  \n",
      "8999        0        0        0        0  \n",
      "\n",
      "[9000 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert the binary predictions to DataFrame\n",
    "predictions_df = pd.DataFrame(binary_predictions_test, columns=['Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5'])\n",
    "\n",
    "# Concatenate the predictions DataFrame with the original test DataFrame\n",
    "df_test_final = pd.concat([test_df_original, predictions_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_list = []\n",
    "for index, row in df_test_final.iterrows():\n",
    "    intent = ','.join([str(i+1) for i, val in enumerate(row[['Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5']]) if val == 1])\n",
    "    intent_list.append(intent)\n",
    "\n",
    "# Add the 'intent' column to the dataframe\n",
    "df_test_final['intent'] = intent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mf:\\D.S\\Hakton_Digikala\\4\\4.ipynb Cell 58\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/D.S/Hakton_Digikala/4/4.ipynb#Y143sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Drop the individual class columns\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/D.S/Hakton_Digikala/4/4.ipynb#Y143sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_test_final\u001b[39m.\u001b[39;49mdrop([\u001b[39m'\u001b[39;49m\u001b[39mClass 1\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mClass 2\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mClass 3\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mClass 4\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mClass 5\u001b[39;49m\u001b[39m'\u001b[39;49m], axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/D.S/Hakton_Digikala/4/4.ipynb#Y143sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Display the updated test dataframe\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/D.S/Hakton_Digikala/4/4.ipynb#Y143sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df_test_final\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:5568\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5420\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[0;32m   5421\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   5422\u001b[0m     labels: IndexLabel \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5429\u001b[0m     errors: IgnoreRaise \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   5430\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5431\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   5432\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5433\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5566\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5567\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5568\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[0;32m   5569\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   5570\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   5571\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   5572\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   5573\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   5574\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   5575\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   5576\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4782\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4780\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   4781\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 4782\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4784\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   4785\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4824\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4822\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m   4823\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4824\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4825\u001b[0m     indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4827\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4828\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7069\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7067\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[0;32m   7068\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 7069\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlabels[mask]\u001b[39m.\u001b[39mtolist()\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   7070\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[0;32m   7071\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Drop the individual class columns\n",
    "df_test_final.drop(['Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_final.to_csv(\"df_test_final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
